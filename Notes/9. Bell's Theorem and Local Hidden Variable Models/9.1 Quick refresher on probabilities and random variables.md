---
dg-publish: true
---
#Notes 

In this section, we need to use the notion of a **random variable**. 

Random variables are variables which take on different values probabilistically. The probability that a random variable $A$ takes the value $a$ is $P(a)$. 

When we have a pair of random variables, $A$ and $B$, we will have a **joint probability distribution** $P(a,b)$ which tells us the probability that both $A=a$ and $B=b$. 

If we are only interested in one of the variables $A$ or $B$, we use the **marginal probability distributions** $P(a)$ or $P(b)$ where

$$
P(a)=\sum_{b}P(a,b)
$$
etc. where we essentially say what is the probability that $A=a$ given $B$ can take any value. 

The **conditional probability distribution** arises when the value of one random variable is known, and we want to update the probabilities of the remaining variable, given the new information. We denote $P(a|b)$ as the probability that $A=a$ given $B=b$, with a formula

$$
P(a|b)=\frac{P(a,b)}{P(b)}
$$

that is, all events are normalised by dividing by the probability that event $b$ happened. By combining the above two equations, we arrive at the **law of total probability**

$$
P(a)=\sum_{b}P(a|b)P(b)
$$

which says that the total probability for $A=a$ is the average of the conditional probabilities $A=a$ knowing that $B=b$, weighted by the probability that $B=b$.